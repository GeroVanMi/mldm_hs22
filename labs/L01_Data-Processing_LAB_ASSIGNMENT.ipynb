{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "8bPV9aEwTKC8"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 0x0"
   ],
   "metadata": {
    "id": "jFHJbjkfeepf"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TASK 1 (2 Points): \n",
    "\n",
    "We work with the \"Wine Recognition\" dataset. You can read more about this dataset at [https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset).\n",
    "\n",
    "The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators.\n",
    "The data is loaded below and split into `data` and `target`. `data` is a `Dataframe` that contains the result of the chemical analysis while `target`contains an integer representing the wine cultivator."
   ],
   "metadata": {
    "id": "ykbI8UnR6PsU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_wine\n",
    "(data, target) = load_wine(return_X_y=True, as_frame=True)"
   ],
   "metadata": {
    "id": "em6VCOuE6MRU"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data"
   ],
   "metadata": {
    "id": "HJoAuMNR6MgM"
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0      14.23        1.71  2.43               15.6      127.0           2.80   \n1      13.20        1.78  2.14               11.2      100.0           2.65   \n2      13.16        2.36  2.67               18.6      101.0           2.80   \n3      14.37        1.95  2.50               16.8      113.0           3.85   \n4      13.24        2.59  2.87               21.0      118.0           2.80   \n..       ...         ...   ...                ...        ...            ...   \n173    13.71        5.65  2.45               20.5       95.0           1.68   \n174    13.40        3.91  2.48               23.0      102.0           1.80   \n175    13.27        4.28  2.26               20.0      120.0           1.59   \n176    13.17        2.59  2.37               20.0      120.0           1.65   \n177    14.13        4.10  2.74               24.5       96.0           2.05   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0          3.06                  0.28             2.29             5.64  1.04   \n1          2.76                  0.26             1.28             4.38  1.05   \n2          3.24                  0.30             2.81             5.68  1.03   \n3          3.49                  0.24             2.18             7.80  0.86   \n4          2.69                  0.39             1.82             4.32  1.04   \n..          ...                   ...              ...              ...   ...   \n173        0.61                  0.52             1.06             7.70  0.64   \n174        0.75                  0.43             1.41             7.30  0.70   \n175        0.69                  0.43             1.35            10.20  0.59   \n176        0.68                  0.53             1.46             9.30  0.60   \n177        0.76                  0.56             1.35             9.20  0.61   \n\n     od280/od315_of_diluted_wines  proline  \n0                            3.92   1065.0  \n1                            3.40   1050.0  \n2                            3.17   1185.0  \n3                            3.45   1480.0  \n4                            2.93    735.0  \n..                            ...      ...  \n173                          1.74    740.0  \n174                          1.56    750.0  \n175                          1.56    835.0  \n176                          1.62    840.0  \n177                          1.60    560.0  \n\n[178 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95.0</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740.0</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102.0</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750.0</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835.0</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840.0</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96.0</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "target"
   ],
   "metadata": {
    "id": "xrsPKm3w6Mi-"
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n173    2\n174    2\n175    2\n176    2\n177    2\nName: target, Length: 178, dtype: int32"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "/home/gero/Projects/san-francisco-crimeNext, the data is split into training data and testing data.\n",
    "The training data is used to train the model while the testing data is used to evaluate the model on different data than it was trained for. You will learn later in the course why this is necessary."
   ],
   "metadata": {
    "id": "B3W5r6Se8kXW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)"
   ],
   "metadata": {
    "id": "m1w8dDgw6MoO"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the following, we define functions to classify the data. We use a [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html#tree) and a [Support Vector Classifier](https://scikit-learn.org/stable/modules/svm.html#svm-classification). You will learn later in the course how these classifiers work."
   ],
   "metadata": {
    "id": "J_eeYvZc-f_n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_test_predicted = clf.predict(X_test)\n",
    "  return accuracy_score(y_test, y_test_predicted)\n",
    "\n",
    "\n",
    "def run_decision_tree(X_train, y_train, X_test, y_test):\n",
    "  clf = DecisionTreeClassifier(random_state=0)\n",
    "  accuracy = run_classifier(clf, X_train, y_train, X_test, y_test)\n",
    "  print(\"The accuracy of the Decision Tree classifier is\", accuracy)\n",
    "\n",
    "def run_svc(X_train, y_train, X_test, y_test):\n",
    "  clf = SVC(random_state=0)\n",
    "  accuracy = run_classifier(clf, X_train, y_train, X_test, y_test)\n",
    "  print(\"The accuracy of the Support Vector classifier is\", accuracy)\n"
   ],
   "metadata": {
    "id": "pvm_zBOe-e_X"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1a: Classify the data\n",
    "\n",
    "Classify the data by calling the two functions `run_decision_tree` and `run_svc`.\n",
    "Which classifier works better (i.e. achieves the higher accuracy)?"
   ],
   "metadata": {
    "id": "s1MS2D8LAMpD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "run_decision_tree(X_train, y_train, X_test, y_test)\n",
    "run_svc(X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "id": "5ToW8fx4ANZ8"
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree classifier is 0.9661016949152542\n",
      "The accuracy of the Support Vector classifier is 0.711864406779661\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1b: Normalize the data with mean and standard deviation\n",
    "\n",
    "Normalize the training and testing data using the following formula:\n",
    "\n",
    "$$X_{normalized} = \\frac{X-\\mu_X}{\\sigma_X}$$\n",
    "\n",
    "Calculate the mean and standard deviation __on the training data__ only (also when you normalize the testing dataset).\n",
    "\n",
    "`Pandas` provides built-in functions to calculate the average and the standard deviation. For example, `X_train.mean()` returns the average value per feature in the training dataset while `X_train.std()` returns the standard deviation per feature."
   ],
   "metadata": {
    "id": "BbM8OUZFBRGH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_mean = X_train.mean()\n",
    "x_standard_deviation = X_train.std()\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "y_standard_deviation = y_train.std()"
   ],
   "metadata": {
    "id": "K0qkP9TqBRft"
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "22     13.71        1.86  2.36               16.6      101.0           2.61   \n",
      "146    13.88        5.04  2.23               20.0       80.0           0.98   \n",
      "97     12.29        1.41  1.98               16.0       85.0           2.55   \n",
      "69     12.21        1.19  1.75               16.8      151.0           1.85   \n",
      "167    12.82        3.37  2.30               19.5       88.0           1.48   \n",
      "..       ...         ...   ...                ...        ...            ...   \n",
      "71     13.86        1.51  2.67               25.0       86.0           2.95   \n",
      "106    12.25        1.73  2.12               19.0       80.0           1.65   \n",
      "14     14.38        1.87  2.38               12.0      102.0           3.30   \n",
      "92     12.69        1.53  2.26               20.7       80.0           1.38   \n",
      "102    12.34        2.45  2.46               21.0       98.0           2.56   \n",
      "\n",
      "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "22         2.88                  0.27             1.69             3.80  1.11   \n",
      "146        0.34                  0.40             0.68             4.90  0.58   \n",
      "97         2.50                  0.29             1.77             2.90  1.23   \n",
      "69         1.28                  0.14             2.50             2.85  1.28   \n",
      "167        0.66                  0.40             0.97            10.26  0.72   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "71         2.86                  0.21             1.87             3.38  1.36   \n",
      "106        2.03                  0.37             1.63             3.40  1.00   \n",
      "14         3.64                  0.29             2.96             7.50  1.20   \n",
      "92         1.46                  0.58             1.62             3.05  0.96   \n",
      "102        2.11                  0.34             1.31             2.80  0.80   \n",
      "\n",
      "     od280/od315_of_diluted_wines  proline  \n",
      "22                           4.00   1035.0  \n",
      "146                          1.33    415.0  \n",
      "97                           2.74    428.0  \n",
      "69                           3.07    718.0  \n",
      "167                          1.75    685.0  \n",
      "..                            ...      ...  \n",
      "71                           3.16    410.0  \n",
      "106                          3.17    510.0  \n",
      "14                           3.00   1547.0  \n",
      "92                           2.06    495.0  \n",
      "102                          3.38    438.0  \n",
      "\n",
      "[119 rows x 13 columns]\n",
      "alcohol                          12.971008\n",
      "malic_acid                        2.413361\n",
      "ash                               2.372101\n",
      "alcalinity_of_ash                19.577311\n",
      "magnesium                       100.857143\n",
      "total_phenols                     2.278319\n",
      "flavanoids                        2.014538\n",
      "nonflavanoid_phenols              0.364034\n",
      "proanthocyanins                   1.588739\n",
      "color_intensity                   5.025126\n",
      "hue                               0.956353\n",
      "od280/od315_of_diluted_wines      2.593782\n",
      "proline                         741.588235\n",
      "dtype: float64\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call the two classification functions again with the normalized data and report the changes in accuracy. What do you notice?"
   ],
   "metadata": {
    "id": "_fNuBgC6BSFt"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TFg6WbmgBShk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1c: Repeat Task 1b with min-max Normalization\n",
    "\n",
    "Repeat the task 1b but use the following formula to normalize tha data:\n",
    "\n",
    "$$X_{normalized} = \\frac{X-X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "Again, calculate the mean and standard deviation __on the training data__ only (also when you normalize the testing dataset) and use the built-in function `X_train.min()` resp. `X_train.max()`."
   ],
   "metadata": {
    "id": "1_1EVF-TBS7v"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "i25XenppJ7gf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call the two classification functions again with the normalized data and report the changes in accuracy. What do you notice?"
   ],
   "metadata": {
    "id": "NIy0ECbTJ7gq"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "99uuR7ngJ7gr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📢 **HAND-IN** 📢: Report on Moodle whether you solved this task."
   ],
   "metadata": {
    "id": "c_i1aBh6KnWw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# TASK 2 (2 Points): \n",
    "\n",
    "In Task 1 we clearly saw that normalization improves the result for Support Vector Classifiers but not for Decision Trees. You will learn later in the course why Decision Trees don't need normalization.\n",
    "\n",
    "However, to better understand the influence of normalization, we will plot the data with and without normalization.\n"
   ],
   "metadata": {
    "id": "m7I1RBjQK7Ly"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")"
   ],
   "metadata": {
    "id": "w9qp3e4nBTPK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2a: Plot the unnormalized data\n",
    "\n",
    "For simplicity, we only consider only the columns `alcohol` and `malic_acid` from the training dataset.\n",
    "\n",
    "Create a [Scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) from the data with  the attribute `alcohol` on the `x`-axis and `malic_acid` on the `y`-axis.\n",
    "\n",
    "Plot the un-normalized data `X_train` as well as the two noramlized versions from Exercise 1 in the same plot and describe what happens.\n",
    "\n",
    "__Hint:__ To visualize the data distribution in the same plot just call `sns.scatterplot` three times within the same code-cell."
   ],
   "metadata": {
    "id": "tnF26SbCNCRS"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-lc07hbiOvYu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now have a closer look at the data. Calculate for the un-normalized data as well as for the two normalized versions of data\n",
    "\n",
    "- The average value in the column `avg(alcohol)`\n",
    "- The standard deviation in the column `std(alcohol)`\n",
    "- The minimum value in the column `min(alcohol)`\n",
    "- The maxmium value in the column `max(alcohol)`\n",
    "- The range in the column by subtracting the minimum of the maximum in the column `max(alcohol) - min(alcohol)`\n",
    "\n",
    "Compare the properties of the un-normalized data with the normalized data. What do you notice?"
   ],
   "metadata": {
    "id": "pJ5Ncd5cN-9z"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "J3D06pyKQjGq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📢 **HAND-IN** 📢: Report on Moodle whether you solved this task.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "AH7H07ZcSniv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TASK 3 (6 Points): Binning\n",
    "\n"
   ],
   "metadata": {
    "id": "UT3_BLJDl-0o"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7K4Cikz4aZE"
   },
   "source": [
    "The following list consists of the age of several people: \n",
    "```python\n",
    "[13, 15, 16, 18, 19, 20, 20, 21, 22, 22, 25, 25, 26, 26, 30, 33, 34, 35, 35, 35, 36, 37, 40, 42, 46, 53, 70]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsHmNGlW4aZE"
   },
   "source": [
    "### Task 3a: Equal-Width Binning\n",
    "Apply binning to the dataset using 3 equal-width bins. Smooth the data using the mean of the bins.\n",
    "\n",
    "Tips:\n",
    "1. Calculate the size of the bins\n",
    "2. Assign each value to the corresponding bin\n",
    "3. Calculate the mean per bin\n",
    "4. Replace each value by the mean of its bin\n",
    "\n",
    "__Solve this exercise by hand without using Python__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eukBUnVs4aZE"
   },
   "source": [
    "❗ TODO ❗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UL9OUG44aZF"
   },
   "source": [
    "###Task 3b: Equal-Depth Binning\n",
    "\n",
    "Apply binning to the dataset using 3 equal-depth bins. Smooth the data using the mean of the bins. Explain the steps of your approach and give the final result.\n",
    "\n",
    "Tips:\n",
    "1. Calculate the number of elements per bin\n",
    "2. Assign each value to the corresponding bin\n",
    "3. Calculate the mean per bin\n",
    "4. Replace each value by the mean of its bin\n",
    "\n",
    "__Please solve this exercise by hand without using Python__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vhf3wkSm4aZF"
   },
   "source": [
    "\n",
    "❗ TODO ❗"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📢 **HAND-IN** 📢: Describe on Moodle the results of Exercise 3: \n",
    "\n",
    "* Copy the results of Exercise 3a and 3b to Moodle\n",
    "* Describe the differences between task 3a and task 3b\n",
    "* Describe situations when binning should be used and give a concrete example. Are there also circumstances in which binning should not be applied?"
   ],
   "metadata": {
    "id": "ex21HuPTl_Qx"
   }
  }
 ]
}
